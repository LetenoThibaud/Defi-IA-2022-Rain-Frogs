{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-09 19:30:59.614950: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-09 19:30:59.615032: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import keras.models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.models\n",
    "import keras.layers as layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Sequential, activations\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner as kt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_normalization_infos(*x_s, columns):\n",
    "    normalization_infos = pd.DataFrame(data=[[1000 for _ in range(len(columns))],[0 for _ in range(len(columns))]],\n",
    "                                       index=[\"min\",\"max\"],\n",
    "                                       columns=columns)\n",
    "    for x in x_s :\n",
    "        for col in columns:\n",
    "            min_value = min(normalization_infos[col][\"min\"], x[col].min())\n",
    "            max_value = max(normalization_infos[col][\"max\"], x[col].max())\n",
    "            normalization_infos[col] = [min_value, max_value]\n",
    "\n",
    "    normalization_infos.loc[\"spread\"] = normalization_infos.apply(lambda c : c[\"max\"] - c[\"min\"], axis=0)\n",
    "\n",
    "    return normalization_infos\n",
    "\n",
    "\n",
    "def normalize(x : pd.DataFrame, normalization_infos : pd.DataFrame):\n",
    "    for col in x.columns:\n",
    "        x[col] = (x[col] - normalization_infos[col][\"min\"])/normalization_infos[col][\"spread\"]\n",
    "    return x\n",
    "\n",
    "\n",
    "def m_mape(y_true, y_predict):\n",
    "    n = len(y_true)\n",
    "    At = np.array(y_true) + 1\n",
    "    Ft = np.array(y_predict) + 1\n",
    "\n",
    "    res = ((100/n)*(np.sum(np.abs((Ft-At)/At))))\n",
    "    return res\n",
    "\n",
    "\n",
    "    # return (100 / len(y_true)) * (np.sum(np.abs((np.array(y_predict) + 1 - np.array(y_true) + 1) / np.array(y_true) + 1)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def create_mlp_regression(x_train, nb_hidden_layers=0, final_activation=None):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Dense(30, activation=\"relu\", input_shape=x_train.shape[1:]))\n",
    "    # model.add(layers.Dropout(0.1))\n",
    "    if nb_hidden_layers > 0:\n",
    "        for i in range(nb_hidden_layers):\n",
    "            model.add(layers.Dense(30, activation=\"relu\", input_shape=x_train.shape[1:]))\n",
    "            # model.add(layers.Dropout(0.1))\n",
    "    if final_activation is None:\n",
    "        model.add(layers.Dense(1))\n",
    "    else :\n",
    "        exp_relu = lambda x : activations.exponential(activations.relu(x)) - 1\n",
    "        model.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "    # default = learning_rate=0.01, momentum=0.0, nesterov=False\n",
    "    # momentum: float hyperparameter >= 0 that accelerates gradient descent in the relevant direction and dampens oscillations.\n",
    "    # nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0, nesterov=False)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code for Submission"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "cols_to_drop = {\"day\",\"Id\", \"station_id\",\"month\",\"longitude_idx\",\"latitude_idx\", \"next hour precipitation (kg/m^2)\", \"wind_direction (deg)\"}\n",
    "\n",
    "df_2016 = pd.read_csv(\"./preprocessed_data_Defi-IA-2022-Rain-Frogs/X_all_2016_final_by_day.zip\")\n",
    "df_2017 = pd.read_csv(\"./preprocessed_data_Defi-IA-2022-Rain-Frogs/X_all_2017_final_by_day.zip\")\n",
    "df_train = pd.concat([df_2016, df_2017], axis=0)\n",
    "\n",
    "del df_2017\n",
    "del df_2016\n",
    "\n",
    "# df_train[\"raining\"] = df_train[\"ground_truth\"].apply(lambda x : 0 if (x==0) else 1)\n",
    "x_train, x_valid, y_reg_train, y_reg_valid = train_test_split(df_train.drop([\"ground_truth\"], axis=1), df_train[\"ground_truth\"], test_size=0.2) # , df_train[\"raining\"]\n",
    "#drop unnecessary columns\n",
    "x_train.drop(list(set(x_train.columns)&cols_to_drop), axis=1, inplace=True)\n",
    "x_valid.drop(list(set(x_train.columns)&cols_to_drop), axis=1, inplace=True)\n",
    "\n",
    "y_reg_train = np.float32(y_reg_train)\n",
    "y_reg_valid = np.float32(y_reg_valid)\n",
    "\n",
    "del df_train\n",
    "\n",
    "# Evaluation set\n",
    "\n",
    "x_test = pd.read_csv(\"./preprocessed_data_Defi-IA-2022-Rain-Frogs/X_all_test_final_by_day.zip\")\n",
    "submission = pd.DataFrame(x_test[[\"Id\"]])\n",
    "\n",
    "columns = list(x_train.columns)\n",
    "x_train = x_train[columns]\n",
    "x_test = x_test[columns]\n",
    "x_valid = x_valid[columns]\n",
    "\n",
    "# Normalization\n",
    "normalization_infos = get_normalization_infos(x_train, x_test, columns=columns)\n",
    "x_train = normalize(x_train, normalization_infos)\n",
    "x_test = normalize(x_test, normalization_infos)\n",
    "x_valid = normalize(x_valid, normalization_infos)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_108 (Dense)           (None, 30)                2940      \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 30)                930       \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 30)                930       \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 30)                930       \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 30)                930       \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 30)                930       \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,621\n",
      "Trainable params: 7,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 2/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 3/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 4/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 5/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 6/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 7/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 8/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 9/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 10/75\n",
      "5699/5699 [==============================] - 13s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 11/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 12/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 13/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 14/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 15/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 16/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 17/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 18/75\n",
      "5699/5699 [==============================] - 13s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 19/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 20/75\n",
      "5699/5699 [==============================] - 12s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 21/75\n",
      "5699/5699 [==============================] - 8s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 22/75\n",
      "5699/5699 [==============================] - 9s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 23/75\n",
      "5699/5699 [==============================] - 10s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 24/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 25/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 26/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 27/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 28/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 29/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 30/75\n",
      "5699/5699 [==============================] - 12s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 31/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 32/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 33/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 34/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 35/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 36/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 37/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 38/75\n",
      "5699/5699 [==============================] - 9s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 39/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 40/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 41/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 42/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 43/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 44/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 45/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 46/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 47/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 48/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 49/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 50/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 51/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 52/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 53/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 54/75\n",
      "5699/5699 [==============================] - 7s 1ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 55/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 56/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 57/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 58/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 59/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 60/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 61/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 62/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 63/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 64/75\n",
      "5699/5699 [==============================] - 12s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 65/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 66/75\n",
      "5699/5699 [==============================] - 12s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 67/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 68/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 69/75\n",
      "5699/5699 [==============================] - 11s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 70/75\n",
      "5699/5699 [==============================] - 12s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 71/75\n",
      "5699/5699 [==============================] - 12s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 72/75\n",
      "5699/5699 [==============================] - 13s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 73/75\n",
      "5699/5699 [==============================] - 19s 3ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 74/75\n",
      "5699/5699 [==============================] - 18s 3ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 75/75\n",
      "5699/5699 [==============================] - 14s 2ms/step - loss: 0.0391 - val_loss: 0.0395\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "model = create_mlp_regression(x_train, nb_hidden_layers=5, final_activation='relu')\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_reg_train, epochs=75, validation_data=(x_valid,y_reg_valid), verbose=True)\n",
    "y_pred = np.concatenate(model.predict(x_test.values), axis=0)\n",
    "y_pred = y_pred + 1\n",
    "\n",
    "print(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    85140.000000\n",
      "mean         1.083525\n",
      "std          0.081641\n",
      "min          1.000000\n",
      "25%          1.024015\n",
      "50%          1.060697\n",
      "75%          1.122318\n",
      "max          1.997445\n",
      "Name: Prediction, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('./submission_ann_regression_75_square_relu.csv')\n",
    "print(submission['Prediction'].describe())\n",
    "submission[[\"Id\",\"Prediction\"]].to_csv(\"./submission_ann_regression_75_epochs.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code for experiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "cols_to_drop = {\"day\",\"Id\", \"station_id\",\"month\",\"longitude_idx\",\"latitude_idx\", \"next hour precipitation (kg/m^2)\", \"wind_direction (deg)\"}\n",
    "\n",
    "df_train = pd.read_csv(\"./preprocessed_data_Defi-IA-2022-Rain-Frogs/X_all_2016_final_by_day.zip\")\n",
    "df_train[\"raining\"] = df_train[\"ground_truth\"].apply(lambda x : 0 if (x==0) else 1)\n",
    "df_train[\"not_raining\"] = 1-df_train[\"raining\"]\n",
    "x_train, y_reg_train, y_cls_train = df_train.drop([\"ground_truth\",\"raining\"], axis=1), df_train[\"ground_truth\"], df_train[[\"not_raining\",\"raining\"]].to_numpy()\n",
    "#drop unnecessary columns\n",
    "x_train.drop(list(set(x_train.columns)&cols_to_drop), axis=1, inplace=True)\n",
    "del df_train\n",
    "\n",
    "df_test = pd.read_csv(\"./preprocessed_data_Defi-IA-2022-Rain-Frogs/X_all_2017_final_by_day.zip\")\n",
    "df_test[\"raining\"] = df_test[\"ground_truth\"].apply(lambda x : 0 if (x==0) else 1)\n",
    "df_test[\"not_raining\"] = 1-df_test[\"raining\"]\n",
    "x_test, y_reg_test, y_cls_test = df_test.drop([\"ground_truth\",\"raining\"], axis=1), df_test[\"ground_truth\"], df_test[[\"not_raining\",\"raining\"]].to_numpy()\n",
    "#drop unnecessary columns\n",
    "x_test.drop(list(set(x_test.columns)&cols_to_drop), axis=1, inplace=True)\n",
    "del df_test\n",
    "\n",
    "columns = list(x_train.columns)\n",
    "x_train = x_train[columns]\n",
    "x_test = x_test[columns]\n",
    "\n",
    "normalization_infos = get_normalization_infos(x_train, x_test, columns=columns)\n",
    "\n",
    "x_train = normalize(x_train, normalization_infos)\n",
    "x_test = normalize(x_test, normalization_infos)\n",
    "\n",
    "y_reg_train = np.float32(y_reg_train)\n",
    "y_reg_test = np.float32(y_reg_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "\"\"\"def create_mlp_classification(x_train, nb_hidden_layers=0):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Dense(30, activation=\"relu\", input_shape=x_train.shape[1:]))\n",
    "    # model.add(layers.Dropout(0.1))\n",
    "    if nb_hidden_layers > 0:\n",
    "        for i in range(nb_hidden_layers):\n",
    "            model.add(layers.Dense(30, activation=\"relu\", input_shape=x_train.shape[1:]))\n",
    "            # model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    # model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
    "    return model\"\"\"\n",
    "\n",
    "# Creating model using the Sequential in tensorflow\n",
    "def build_model_classf(*layers, loss ='binary_crossentropy', metrics = (\"AUC\",\"accuracy\"), learning_rate=0.001, last_activation=\"softmax\"):\n",
    "    model = Sequential()\n",
    "    model.add(layers[0])\n",
    "    for layer in layers[1:]:\n",
    "        model.add(layer)\n",
    "    model.add(Dense(2, activation=last_activation))\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def make_cls_model(*layers,x,y,x_val=None,y_val=None,val_split=0.15, epochs=15, batch_size=64, learning_rate=0.001, last_activation=\"softmax\"):\n",
    "    # build the model\n",
    "    model_cls = build_model_classf(*layers, learning_rate=learning_rate, last_activation=last_activation)\n",
    "    # train the model\n",
    "    if x_val is None :\n",
    "        history_cls = model_cls.fit(\n",
    "            x=x.values,\n",
    "            y=y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=val_split\n",
    "            # validation_data=(x_test,y_cls_test)\n",
    "        )\n",
    "    else :\n",
    "        history_cls = model_cls.fit(\n",
    "            x=x.values,\n",
    "            y=y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            # validation_split=0.15\n",
    "            validation_data=(x_val,y_val)\n",
    "        )\n",
    "    return history_cls,model_cls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_279 (Dense)           (None, 128)               12672     \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_280 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_281 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_283 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_284 (Dense)           (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,626\n",
      "Trainable params: 24,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "788/788 [==============================] - 4s 4ms/step - loss: 0.0369 - auc: 0.9998 - accuracy: 0.9902 - val_loss: 4.2682e-06 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "788/788 [==============================] - 2s 3ms/step - loss: 2.5348e-05 - auc: 1.0000 - accuracy: 1.0000 - val_loss: 9.5675e-07 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "788/788 [==============================] - 3s 3ms/step - loss: 8.3750e-06 - auc: 1.0000 - accuracy: 1.0000 - val_loss: 3.5969e-07 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "788/788 [==============================] - 2s 3ms/step - loss: 2.1941e-06 - auc: 1.0000 - accuracy: 1.0000 - val_loss: 1.4829e-07 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "788/788 [==============================] - 3s 3ms/step - loss: 0.0276 - auc: 0.9975 - accuracy: 0.9956 - val_loss: 1.0463e-06 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_279 (Dense)           (None, 128)               12672     \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_280 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_281 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_283 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_284 (Dense)           (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,626\n",
      "Trainable params: 24,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3151/3151 [==============================] - 8s 2ms/step - loss: 0.0012 - auc: 0.9999 - accuracy: 0.9997 - val_loss: 3.3786e-07 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "[1.0000000e+00 1.8846118e-12 6.4318044e-16 ... 1.0000000e+00 1.1765632e-15\n",
      " 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# model_cls = create_mlp_classification(x_train, nb_hidden_layers=5)\n",
    "history_cls, model_cls = make_cls_model(\n",
    "    Dense(128, activation=\"relu\",input_dim=x_train.shape[1]),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    # Dropout(0.2),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    # Dropout(0.2),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    # Dropout(0.2),\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    x=x_train,y=y_cls_train,\n",
    "    epochs=5,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "model_cls.summary()\n",
    "history_cls = model_cls.fit(x_train.values, y_cls_train, validation_split=0.15, verbose=True)\n",
    "y_pred_cls = np.concatenate(model_cls.predict(x_test.values), axis=0 )\n",
    "print(int(y_pred_cls))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_299 (Dense)           (None, 80)                7920      \n",
      "                                                                 \n",
      " dense_300 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_301 (Dense)           (None, 70)                5670      \n",
      "                                                                 \n",
      " dense_302 (Dense)           (None, 60)                4260      \n",
      "                                                                 \n",
      " dense_303 (Dense)           (None, 50)                3050      \n",
      "                                                                 \n",
      " dense_304 (Dense)           (None, 40)                2040      \n",
      "                                                                 \n",
      " dense_305 (Dense)           (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,461\n",
      "Trainable params: 29,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "3151/3151 [==============================] - 7s 2ms/step - loss: 0.0295 - val_loss: 0.0252\n",
      "Epoch 2/15\n",
      "3151/3151 [==============================] - 6s 2ms/step - loss: 0.0275 - val_loss: 0.0248\n",
      "Epoch 3/15\n",
      "3151/3151 [==============================] - 5s 2ms/step - loss: 0.0273 - val_loss: 0.0246\n",
      "Epoch 4/15\n",
      "3151/3151 [==============================] - 5s 2ms/step - loss: 0.0271 - val_loss: 0.0245\n",
      "Epoch 5/15\n",
      "3151/3151 [==============================] - 7s 2ms/step - loss: 0.0269 - val_loss: 0.0240\n",
      "Epoch 6/15\n",
      "3151/3151 [==============================] - 10s 3ms/step - loss: 0.0267 - val_loss: 0.0239\n",
      "Epoch 7/15\n",
      "3151/3151 [==============================] - 8s 2ms/step - loss: 0.0265 - val_loss: 0.0237\n",
      "Epoch 8/15\n",
      "3151/3151 [==============================] - 7s 2ms/step - loss: 0.0264 - val_loss: 0.0235\n",
      "Epoch 9/15\n",
      "3151/3151 [==============================] - 6s 2ms/step - loss: 0.0262 - val_loss: 0.0236\n",
      "Epoch 10/15\n",
      "3151/3151 [==============================] - 6s 2ms/step - loss: 0.0260 - val_loss: 0.0241\n",
      "Epoch 11/15\n",
      "3151/3151 [==============================] - 7s 2ms/step - loss: 0.0259 - val_loss: 0.0232\n",
      "Epoch 12/15\n",
      "3151/3151 [==============================] - 7s 2ms/step - loss: 0.0257 - val_loss: 0.0230\n",
      "Epoch 13/15\n",
      "3151/3151 [==============================] - 6s 2ms/step - loss: 0.0255 - val_loss: 0.0225\n",
      "Epoch 14/15\n",
      "3151/3151 [==============================] - 7s 2ms/step - loss: 0.0254 - val_loss: 0.0226\n",
      "Epoch 15/15\n",
      "3151/3151 [==============================] - 6s 2ms/step - loss: 0.0252 - val_loss: 0.0238\n",
      "nb_hidden_layer 5 mape 5.5756531414937065\n"
     ]
    }
   ],
   "source": [
    "model = create_mlp_regression(x_train, nb_hidden_layers=5, final_activation=\"relu\")\n",
    "model.summary()\n",
    "history = model.fit(x_train.values, y_reg_train.values, epochs=15, validation_split=0.15, verbose=True)\n",
    "y_pred = np.concatenate(model.predict(x_test.values), axis=0 )\n",
    "evaluation_score = m_mape(y_reg_test, y_pred)\n",
    "print('nb_hidden_layer', 5, 'mape', evaluation_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from ./untitled_project/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (30,) and (90,) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_6393/3705429596.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0mtuner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msearch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_reg_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_test\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_reg_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m \u001B[0mbest_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtuner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_best_models\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\u001B[0m in \u001B[0;36mget_best_models\u001B[0;34m(self, num_models)\u001B[0m\n\u001B[1;32m    389\u001B[0m         \"\"\"\n\u001B[1;32m    390\u001B[0m         \u001B[0;31m# Method only exists in this class for the docstring override.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 391\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTuner\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_best_models\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_models\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    392\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    393\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_deepcopy_callbacks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py\u001B[0m in \u001B[0;36mget_best_models\u001B[0;34m(self, num_models)\u001B[0m\n\u001B[1;32m    280\u001B[0m         \"\"\"\n\u001B[1;32m    281\u001B[0m         \u001B[0mbest_trials\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moracle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_best_trials\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_models\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 282\u001B[0;31m         \u001B[0mmodels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mtrial\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mbest_trials\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    283\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodels\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    280\u001B[0m         \"\"\"\n\u001B[1;32m    281\u001B[0m         \u001B[0mbest_trials\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moracle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_best_trials\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_models\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 282\u001B[0;31m         \u001B[0mmodels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mtrial\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mbest_trials\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    283\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodels\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\u001B[0m in \u001B[0;36mload_model\u001B[0;34m(self, trial)\u001B[0m\n\u001B[1;32m    327\u001B[0m         \u001B[0mbest_epoch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrial\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_step\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mmaybe_distribute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistribution_strategy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 329\u001B[0;31m             model.load_weights(\n\u001B[0m\u001B[1;32m    330\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_checkpoint_fname\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrial_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbest_epoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    331\u001B[0m             )\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     68\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py\u001B[0m in \u001B[0;36massert_is_compatible_with\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   1169\u001B[0m     \"\"\"\n\u001B[1;32m   1170\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_compatible_with\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mother\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1171\u001B[0;31m       \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Shapes %s and %s are incompatible\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mother\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1173\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mmost_specific_compatible_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mother\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Shapes (30,) and (90,) are incompatible"
     ]
    }
   ],
   "source": [
    "# pip install keras-tuner --upgrade\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    hp_number_layers = hp.Int('num_layers', min_value=1, max_value=5, step=1)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2]) # , 1e-3, 1e-4])\n",
    "\n",
    "    model = Sequential()\n",
    "    for i in range(hp_number_layers):\n",
    "        model.add(layers.Dense(units=30, activation=\"relu\", input_shape=x_train.shape[1:]))\n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=hp_learning_rate)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5)\n",
    "\n",
    "tuner.search(x_train.values, y_reg_train, epochs=5, validation_data=(x_test.values, y_reg_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}